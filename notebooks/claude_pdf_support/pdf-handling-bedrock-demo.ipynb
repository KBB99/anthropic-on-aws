{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Handling and Citations with AWS Bedrock\n",
    "\n",
    "This notebook demonstrates how to work with PDFs and extract citations using Claude models on AWS Bedrock.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Understanding PDF Handling in Bedrock](#understanding-pdf-handling)\n",
    "2. [Prerequisites and Setup](#prerequisites)\n",
    "3. [Basic PDF Processing](#basic-pdf-processing)\n",
    "4. [Working with Citations](#citations)\n",
    "5. [Practical Examples with Sample PDF](#examples)\n",
    "6. [Advanced Techniques](#advanced)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AWS account with Bedrock access\n",
    "- AWS credentials configured\n",
    "- Required Python packages installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install boto3 PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding PDF Handling in Bedrock <a id=\"understanding-pdf-handling\"></a>\n",
    "\n",
    "### How PDF Handling Works\n",
    "\n",
    "Amazon Bedrock's PDF handling capability allows Claude models to directly process PDF documents in their native format. Here's how it works:\n",
    "\n",
    "1. **Native PDF Processing**: The PDF is sent as a base64-encoded binary to the model\n",
    "2. **Full Document Understanding**: Claude can see and understand:\n",
    "   - Text content with preserved formatting\n",
    "   - Document structure (headers, paragraphs, lists)\n",
    "   - Tables and their relationships\n",
    "   - Images and diagrams within the PDF\n",
    "   - Page layout and organization\n",
    "\n",
    "### Benefits Over Text Extraction\n",
    "\n",
    "Traditional approaches often extract text from PDFs before sending to language models. Native PDF handling offers significant advantages:\n",
    "\n",
    "#### 1. **Preserved Context and Layout**\n",
    "- Maintains spatial relationships between elements\n",
    "- Preserves table structures and formatting\n",
    "- Retains connection between images and their captions\n",
    "- Keeps headers, footers, and page numbers in context\n",
    "\n",
    "#### 2. **Visual Understanding**\n",
    "- Can interpret charts, graphs, and diagrams\n",
    "- Understands visual hierarchies and emphasis\n",
    "- Recognizes formatting cues (bold, italic, font sizes)\n",
    "- Can reference specific page locations\n",
    "\n",
    "#### 3. **Accurate Citations**\n",
    "- Provides exact page numbers for references\n",
    "- Can quote specific passages with their locations\n",
    "- Maintains document integrity for compliance needs\n",
    "- Enables precise source attribution\n",
    "\n",
    "#### 4. **Reduced Preprocessing**\n",
    "- No need for complex PDF parsing libraries\n",
    "- Eliminates text extraction errors\n",
    "- Handles complex layouts automatically\n",
    "- Works with scanned documents (with good OCR)\n",
    "\n",
    "#### 5. **Better Comprehension**\n",
    "- Understands document flow and structure\n",
    "- Can navigate between sections effectively\n",
    "- Recognizes document types (reports, papers, forms)\n",
    "- Maintains mathematical formulas and special characters\n",
    "\n",
    "### When to Use PDF Handling\n",
    "\n",
    "Native PDF handling is ideal for:\n",
    "- Academic papers and research documents\n",
    "- Legal documents requiring precise citations\n",
    "- Technical manuals with diagrams\n",
    "- Financial reports with complex tables\n",
    "- Multi-column layouts\n",
    "- Documents with mixed content types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration <a id=\"prerequisites\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bedrock client initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "import base64\n",
    "import io\n",
    "import os\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "from IPython.display import IFrame, display\n",
    "\n",
    "# Configure the Bedrock client\n",
    "def setup_bedrock_client(region='us-east-1'):\n",
    "    \"\"\"Setup Bedrock client with retry configuration\"\"\"\n",
    "    config = Config(\n",
    "        region_name=region,\n",
    "        retries=dict(max_attempts=1000)\n",
    "    )\n",
    "    \n",
    "    client = boto3.client(\n",
    "        service_name='bedrock-runtime',\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    return client\n",
    "\n",
    "# Utility function for displaying metrics\n",
    "def display_metrics(response_body, description=\"\"):\n",
    "    \"\"\"Display usage metrics including prompt caching information\"\"\"\n",
    "    usage = response_body.get('usage', {})\n",
    "    \n",
    "    print(f\"\\n{description} Metrics:\")\n",
    "    print(f\"  Input tokens: {usage.get('input_tokens', 0)}\")\n",
    "    print(f\"  Output tokens: {usage.get('output_tokens', 0)}\")\n",
    "    print(f\"  Total tokens: {usage.get('input_tokens', 0) + usage.get('output_tokens', 0)}\")\n",
    "    \n",
    "    # Check for prompt caching metrics\n",
    "    cache_creation_input_tokens = usage.get('cache_creation_input_tokens', 0)\n",
    "    cache_read_input_tokens = usage.get('cache_read_input_tokens', 0)\n",
    "    \n",
    "    if cache_creation_input_tokens > 0:\n",
    "        print(f\"  Cache creation tokens: {cache_creation_input_tokens}\")\n",
    "    if cache_read_input_tokens > 0:\n",
    "        print(f\"  Cache read tokens: {cache_read_input_tokens}\")\n",
    "        print(f\"  Cache hit rate: {cache_read_input_tokens / usage.get('input_tokens', 1) * 100:.1f}%\")\n",
    "\n",
    "# Initialize the client\n",
    "bedrock_client = setup_bedrock_client()\n",
    "print(\"Bedrock client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start: Basic Invoke and Converse Examples\n",
    "\n",
    "Before diving into the full implementation, let's start with simple examples showing both `invoke` and `converse` API calls with PDF handling.\n",
    "\n",
    "**Note**: The examples below show the correct format for both APIs. The key difference is in how documents are structured in the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Basic Invoke Example ===\n",
      "Model Response:\n",
      "This document is a technical report (TR-2024-001) titled \"Advanced Language Models: Performance Analysis and Implementation Guidelines\" published by an AI Research Division in June 2025.\n",
      "\n",
      "**Brief Summary:**\n",
      "\n",
      "The report presents a comprehensive evaluation of four leading language models (GPT-3.5, Claude 2, Claude 3, and GPT-4) across 50 diverse tasks. The study focuses on performance metrics, implementation best practices, and practical applications for organizations.\n",
      "\n",
      "**Key Findings:**\n",
      "- Claude 3 achieved the highest accuracy at 92% for complex reasoning tasks\n",
      "- Modern language models showed 15% improvement in accuracy over previous generations\n",
      "- 2.3x increase in processing efficiency when properly optimized\n",
      "- Token optimization can reduce costs by up to 40%\n",
      "- Proper prompt engineering increases success rates by 25%\n",
      "- Hybrid approaches work best for enterprise applications\n",
      "\n",
      "**Main Content Areas:**\n",
      "1. **Performance Analysis** - Detailed comparison of model accuracy, latency, throughput, and cost efficiency\n",
      "2. **Implementation Guidelines** - Best practices covering model selection, prompt engineering, cost optimization, quality assurance, and security\n",
      "3. **Technical Specifications** -\n",
      "\n",
      "Basic Invoke Metrics:\n",
      "  Input tokens: 16\n",
      "  Output tokens: 256\n",
      "  Total tokens: 272\n",
      "  Cache creation tokens: 5622\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Basic Invoke API call with PDF\n",
    "def basic_invoke_example(pdf_path):\n",
    "    \"\"\"\n",
    "    Demonstrates a simple invoke call with PDF handling and metrics tracking.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encode the PDF\n",
    "    with open(pdf_path, \"rb\") as pdf_file:\n",
    "        encoded_pdf = base64.b64encode(pdf_file.read()).decode(\"utf-8\")\n",
    "    \n",
    "    # Prepare the request\n",
    "    request_body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"document\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"application/pdf\",\n",
    "                            \"data\": encoded_pdf\n",
    "                        },\n",
    "                        \"title\": \"Technical Report\",\n",
    "                        \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"What is this document about? Please provide a brief summary.\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 256\n",
    "    }\n",
    "    \n",
    "    # Make the invoke call\n",
    "    try:\n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=\"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "            contentType='application/json',\n",
    "            accept='application/json',\n",
    "            body=json.dumps(request_body)\n",
    "        )\n",
    "        \n",
    "        # Parse and display the response\n",
    "        response_body = json.loads(response['body'].read())\n",
    "        \n",
    "        print(\"=== Basic Invoke Example ===\")\n",
    "        print(\"Model Response:\")\n",
    "        for block in response_body.get('content', []):\n",
    "            if block.get('type') == 'text':\n",
    "                print(block.get('text', ''))\n",
    "        \n",
    "        # Display metrics with prompt caching information\n",
    "        display_metrics(response_body, \"Basic Invoke\")\n",
    "        \n",
    "        return response_body\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run the basic invoke example\n",
    "if os.path.exists(\"sample_technical_report.pdf\"):\n",
    "    invoke_response = basic_invoke_example(\"sample_technical_report.pdf\")\n",
    "else:\n",
    "    print(\"Please ensure sample_technical_report.pdf is in the current directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Basic Converse Example ===\n",
      "Model Response:\n",
      "The main topic of this document is **Advanced Language Models: Performance Analysis and Implementation Guidelines**.\n",
      "\n",
      "## Brief Summary\n",
      "\n",
      "This technical report (TR-2024-001) from an AI Research Division provides a comprehensive evaluation of state-of-the-art language models and practical guidance for their implementation. \n",
      "\n",
      "**Key aspects covered:**\n",
      "\n",
      "- **Performance Analysis**: Comparative evaluation of four leading language models (GPT-3.5, Claude 2, Claude 3, and GPT-4) across 50 diverse tasks, measuring accuracy, latency, throughput, and cost efficiency\n",
      "\n",
      "- **Key Findings**: \n",
      "  - Claude 3 achieved the highest accuracy (92%) in complex reasoning tasks\n",
      "  - Token optimization can reduce costs by up to 40%\n",
      "  - Proper prompt engineering improves success rates by 25%\n",
      "  - Overall 15% improvement in accuracy and 2.3x increase in processing efficiency over previous generations\n",
      "\n",
      "- **Implementation Guidelines**: Best practices for production deployment including model selection, prompt engineering, cost optimization, quality assurance, and security considerations\n",
      "\n",
      "The report aims to help organizations effectively implement language model technologies by providing evidence-based recommendations and actionable insights for maximizing performance while managing costs and maintaining quality standards.\n",
      "\n",
      "Converse API Metrics:\n",
      "  Input tokens: 934\n",
      "  Output tokens: 265\n",
      "  Total tokens: 1199\n",
      "  Cache read tokens: 0\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Basic Converse API call with PDF\n",
    "def basic_converse_example(pdf_path):\n",
    "    \"\"\"\n",
    "    Demonstrates a simple converse call with PDF handling and metrics tracking.\n",
    "    The converse API provides a more conversational interface.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the PDF as raw bytes (not base64 encoded)\n",
    "    with open(pdf_path, \"rb\") as pdf_file:\n",
    "        pdf_bytes = pdf_file.read()\n",
    "    \n",
    "    # Prepare the message - note the different format for converse\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"document\": {\n",
    "                        \"name\": \"Technical Report\",\n",
    "                        \"format\": \"pdf\",\n",
    "                        \"source\": {\n",
    "                            \"bytes\": pdf_bytes  # Raw bytes, not base64\n",
    "                        },\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"text\": \"What is the main topic of this document? Please provide a brief summary.\"\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Make the converse call\n",
    "    try:\n",
    "        response = bedrock_client.converse(\n",
    "            modelId=\"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "            messages=messages,\n",
    "            inferenceConfig={\n",
    "                \"maxTokens\": 512,\n",
    "                \"temperature\": 0.5\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(\"\\n=== Basic Converse Example ===\")\n",
    "        print(\"Model Response:\")\n",
    "        \n",
    "        # Extract and display the response\n",
    "        output = response.get('output', {})\n",
    "        if 'message' in output:\n",
    "            content = output['message'].get('content', [])\n",
    "            for item in content:\n",
    "                if 'text' in item:\n",
    "                    print(item['text'])\n",
    "        \n",
    "        # Show usage information with prompt caching metrics\n",
    "        usage = response.get('usage', {})\n",
    "        print(f\"\\nConverse API Metrics:\")\n",
    "        print(f\"  Input tokens: {usage.get('inputTokens', 0)}\")\n",
    "        print(f\"  Output tokens: {usage.get('outputTokens', 0)}\")\n",
    "        print(f\"  Total tokens: {usage.get('totalTokens', 0)}\")\n",
    "        \n",
    "        # Check for prompt caching in converse API (if available)\n",
    "        if 'cacheCreationInputTokens' in usage:\n",
    "            print(f\"  Cache creation tokens: {usage.get('cacheCreationInputTokens', 0)}\")\n",
    "        if 'cacheReadInputTokens' in usage:\n",
    "            cache_read = usage.get('cacheReadInputTokens', 0)\n",
    "            print(f\"  Cache read tokens: {cache_read}\")\n",
    "            if cache_read > 0:\n",
    "                print(f\"  Cache hit rate: {cache_read / usage.get('inputTokens', 1) * 100:.1f}%\")\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run the basic converse example\n",
    "if os.path.exists(\"sample_technical_report.pdf\"):\n",
    "    converse_response = basic_converse_example(\"sample_technical_report.pdf\")\n",
    "else:\n",
    "    print(\"Please ensure sample_technical_report.pdf is in the current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Invoke vs Converse APIs\n",
    "\n",
    "Amazon Bedrock provides two main APIs for interacting with Claude models:\n",
    "\n",
    "### InvokeModel API\n",
    "- **Purpose**: Direct model invocation with full control over request format\n",
    "- **Use case**: Single-turn interactions, streaming responses, custom formatting\n",
    "- **Format**: Uses Anthropic's message format directly\n",
    "- **Features**: Supports all model features including beta features\n",
    "\n",
    "### Converse API\n",
    "- **Purpose**: Simplified conversational interface\n",
    "- **Use case**: Multi-turn conversations, standardized format across models\n",
    "- **Format**: Uses AWS's unified message format\n",
    "- **Features**: Simplified but may not support all beta features immediately\n",
    "\n",
    "### Key Differences in PDF Handling\n",
    "\n",
    "1. **Document Structure**:\n",
    "   - **InvokeModel**: Uses `type: \"document\"` in content array\n",
    "   - **Converse**: Uses `document` object with specific structure\n",
    "\n",
    "2. **PDF Encoding**:\n",
    "   - **InvokeModel**: Requires base64-encoded PDF data in `source.data`\n",
    "   - **Converse**: Requires raw PDF bytes in `source.bytes` (NOT base64!)\n",
    "\n",
    "3. **Response Format**:\n",
    "   - **InvokeModel**: Returns raw model response\n",
    "   - **Converse**: Returns structured AWS response format\n",
    "\n",
    "Choose based on your needs:\n",
    "- Use **InvokeModel** for maximum control and latest features\n",
    "- Use **Converse** for simplified multi-turn conversations\n",
    "\n",
    "### Important Note on PDF Encoding\n",
    "The most common error when using the Converse API with PDFs is passing base64-encoded data instead of raw bytes. Always remember:\n",
    "- **InvokeModel**: `encode_pdf()` → base64 string\n",
    "- **Converse**: `pdf_file.read()` → raw bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic PDF Processing Functions <a id=\"basic-pdf-processing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF processing functions loaded!\n"
     ]
    }
   ],
   "source": [
    "def encode_pdf(pdf_path):\n",
    "    \"\"\"Encode a PDF file to base64\"\"\"\n",
    "    with open(pdf_path, \"rb\") as pdf_file:\n",
    "        encoded_pdf = base64.b64encode(pdf_file.read()).decode(\"utf-8\")\n",
    "    return encoded_pdf\n",
    "\n",
    "def encode_pdf_from_bytes(pdf_bytes):\n",
    "    \"\"\"Encode PDF bytes to base64\"\"\"\n",
    "    return base64.b64encode(pdf_bytes).decode(\"utf-8\")\n",
    "\n",
    "def extract_pdf_pages(pdf_path, start_page=0, end_page=None):\n",
    "    \"\"\"Extract specific pages from a PDF\"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    writer = PdfWriter()\n",
    "    \n",
    "    total_pages = len(reader.pages)\n",
    "    end_page = end_page or total_pages\n",
    "    \n",
    "    for i in range(start_page, min(end_page, total_pages)):\n",
    "        writer.add_page(reader.pages[i])\n",
    "    \n",
    "    output_buffer = io.BytesIO()\n",
    "    writer.write(output_buffer)\n",
    "    output_buffer.seek(0)\n",
    "    \n",
    "    return output_buffer.getvalue()\n",
    "\n",
    "print(\"PDF processing functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple PDF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pdf(pdf_path, question, max_tokens=1024):\n",
    "    \"\"\"Analyze a PDF with a specific question and track metrics\"\"\"\n",
    "    client = bedrock_client\n",
    "    encoded_pdf = encode_pdf(pdf_path)\n",
    "    \n",
    "    request_body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"document\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"application/pdf\",\n",
    "                            \"data\": encoded_pdf\n",
    "                        },\n",
    "                        \"title\": \"Technical Report\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": question\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    \n",
    "    response = client.invoke_model(\n",
    "        modelId=\"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "        contentType='application/json',\n",
    "        accept='application/json',\n",
    "        body=json.dumps(request_body)\n",
    "    )\n",
    "    \n",
    "    response_body = json.loads(response['body'].read())\n",
    "    \n",
    "    # Display metrics inline\n",
    "    display_metrics(response_body, \"PDF Analysis\")\n",
    "    \n",
    "    return response_body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Citations <a id=\"citations\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citation functions loaded!\n"
     ]
    }
   ],
   "source": [
    "def analyze_pdf_with_citations(pdf_path, question, max_tokens=1024):\n",
    "    \"\"\"Analyze a PDF and extract citations with metrics tracking\"\"\"\n",
    "    client = bedrock_client\n",
    "    encoded_pdf = encode_pdf(pdf_path)\n",
    "    \n",
    "    request_body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"document\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"application/pdf\",\n",
    "                            \"data\": encoded_pdf\n",
    "                        },\n",
    "                        \"title\": \"Technical Report\",\n",
    "                        \"citations\": {\n",
    "                            \"enabled\": True\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": question\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    \n",
    "    response = client.invoke_model(\n",
    "        modelId=\"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "        contentType='application/json',\n",
    "        accept='application/json',\n",
    "        body=json.dumps(request_body)\n",
    "    )\n",
    "    \n",
    "    response_body = json.loads(response['body'].read())\n",
    "    \n",
    "    # Display metrics\n",
    "    display_metrics(response_body, \"Citations Analysis\")\n",
    "    \n",
    "    return response_body\n",
    "\n",
    "def format_citations(content):\n",
    "    \"\"\"Extract and format citations from response\"\"\"\n",
    "    citations = []\n",
    "    for block in content:\n",
    "        if block.get('citations'):\n",
    "            for citation in block.get('citations', []):\n",
    "                citations.append({\n",
    "                    'text': citation.get('cited_text', ''),\n",
    "                    'document': citation.get('document_title', ''),\n",
    "                    'start_page': citation.get('start_page_number', 'N/A'),\n",
    "                    'end_page': citation.get('end_page_number', 'N/A')\n",
    "                })\n",
    "    return citations\n",
    "\n",
    "def display_response_with_citations(response):\n",
    "    \"\"\"Display the response text and citations\"\"\"\n",
    "    if not response:\n",
    "        return\n",
    "    \n",
    "    # Extract main text\n",
    "    full_text = \"\"\n",
    "    for block in response.get('content', []):\n",
    "        if block.get('type') == 'text':\n",
    "            full_text += block.get('text', '')\n",
    "    \n",
    "    print(\"Response:\")\n",
    "    print(full_text)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Extract and display citations\n",
    "    citations = format_citations(response.get('content', []))\n",
    "    if citations:\n",
    "        print(\"Citations:\")\n",
    "        for i, citation in enumerate(citations, 1):\n",
    "            print(f\"\\n[{i}] \\\"{citation['text']}\\\"\")\n",
    "            print(f\"    Source: {citation['document']}\")\n",
    "            print(f\"    Pages: {citation['start_page']}-{citation['end_page']}\")\n",
    "    else:\n",
    "        print(\"No citations found.\")\n",
    "\n",
    "print(\"Citation functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Text Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text_with_citations(text_content, question, title=\"Document\", max_tokens=1024):\n",
    "    \"\"\"Analyze text content and extract citations with metrics\"\"\"\n",
    "    client = bedrock_client\n",
    "    \n",
    "    request_body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"document\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"text\",\n",
    "                            \"media_type\": \"text/plain\",\n",
    "                            \"data\": text_content\n",
    "                        },\n",
    "                        \"title\": title,\n",
    "                        \"citations\": {\n",
    "                            \"enabled\": True\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": question\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    \n",
    "    response = client.invoke_model(\n",
    "        modelId=\"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "        contentType='application/json',\n",
    "        accept='application/json',\n",
    "        body=json.dumps(request_body)\n",
    "    )\n",
    "    \n",
    "    response_body = json.loads(response['body'].read())\n",
    "    \n",
    "    # Display metrics\n",
    "    display_metrics(response_body, \"Text Analysis with Citations\")\n",
    "    \n",
    "    return response_body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming function loaded!\n"
     ]
    }
   ],
   "source": [
    "def stream_pdf_analysis(pdf_path, question, max_tokens=1024):\n",
    "    \"\"\"Stream the analysis of a PDF with metrics tracking\"\"\"\n",
    "    client = bedrock_client\n",
    "    encoded_pdf = encode_pdf(pdf_path)\n",
    "    \n",
    "    request_body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"document\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"application/pdf\",\n",
    "                            \"data\": encoded_pdf\n",
    "                        },\n",
    "                        \"title\": \"Technical Report\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": question\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = client.invoke_model_with_response_stream(\n",
    "            modelId=\"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "            contentType='application/json',\n",
    "            accept='application/json',\n",
    "            body=json.dumps(request_body)\n",
    "        )\n",
    "        \n",
    "        # Process the stream and collect metrics\n",
    "        full_response = \"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        for event in response.get('body'):\n",
    "            chunk = event.get('chunk')\n",
    "            if chunk:\n",
    "                chunk_data = json.loads(chunk.get('bytes').decode())\n",
    "                \n",
    "                # Collect text\n",
    "                if chunk_data.get('type') == 'content_block_delta':\n",
    "                    delta = chunk_data.get('delta', {})\n",
    "                    if delta.get('type') == 'text_delta':\n",
    "                        text = delta.get('text', '')\n",
    "                        full_response += text\n",
    "                        print(text, end='', flush=True)\n",
    "                \n",
    "                # Collect metrics from message_stop event\n",
    "                elif chunk_data.get('type') == 'message_stop':\n",
    "                    if 'amazon-bedrock-invocationMetrics' in chunk_data:\n",
    "                        bedrock_metrics = chunk_data['amazon-bedrock-invocationMetrics']\n",
    "                        metrics['inputTokens'] = bedrock_metrics.get('inputTokenCount', 0)\n",
    "                        metrics['outputTokens'] = bedrock_metrics.get('outputTokenCount', 0)\n",
    "                        metrics['totalTokens'] = metrics['inputTokens'] + metrics['outputTokens']\n",
    "                        \n",
    "                        # Check for cache metrics\n",
    "                        if 'cacheCreationInputTokenCount' in bedrock_metrics:\n",
    "                            metrics['cacheCreationTokens'] = bedrock_metrics['cacheCreationInputTokenCount']\n",
    "                        if 'cacheReadInputTokenCount' in bedrock_metrics:\n",
    "                            metrics['cacheReadTokens'] = bedrock_metrics['cacheReadInputTokenCount']\n",
    "        \n",
    "        # Display metrics after streaming\n",
    "        print(\"\\n\\nStreaming Metrics:\")\n",
    "        print(f\"  Input tokens: {metrics.get('inputTokens', 'N/A')}\")\n",
    "        print(f\"  Output tokens: {metrics.get('outputTokens', 'N/A')}\")\n",
    "        print(f\"  Total tokens: {metrics.get('totalTokens', 'N/A')}\")\n",
    "        \n",
    "        if 'cacheCreationTokens' in metrics:\n",
    "            print(f\"  Cache creation tokens: {metrics['cacheCreationTokens']}\")\n",
    "        if 'cacheReadTokens' in metrics:\n",
    "            print(f\"  Cache read tokens: {metrics['cacheReadTokens']}\")\n",
    "            if metrics.get('inputTokens', 0) > 0:\n",
    "                print(f\"  Cache hit rate: {metrics['cacheReadTokens'] / metrics['inputTokens'] * 100:.1f}%\")\n",
    "        \n",
    "        return full_response\n",
    "    \n",
    "    except ClientError as e:\n",
    "        print(f\"Error: {e.response['Error']['Code']} - {e.response['Error']['Message']}\")\n",
    "        return None\n",
    "\n",
    "print(\"Streaming function loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Specific Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pdf_section(pdf_path, start_page, end_page, question):\n",
    "    \"\"\"Analyze a specific section of a PDF with metrics tracking\"\"\"\n",
    "    \n",
    "    # Extract specific pages\n",
    "    pdf_section = extract_pdf_pages(pdf_path, start_page, end_page)\n",
    "    encoded_section = encode_pdf_from_bytes(pdf_section)\n",
    "    \n",
    "    client = bedrock_client\n",
    "    \n",
    "    request_body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"document\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"application/pdf\",\n",
    "                            \"data\": encoded_section\n",
    "                        },\n",
    "                        \"title\": f\"Document (pages {start_page+1}-{end_page})\",\n",
    "                        \"citations\": {\n",
    "                            \"enabled\": True\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": question\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 1024\n",
    "    }\n",
    "\n",
    "    response = client.invoke_model(\n",
    "        modelId=\"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "        contentType='application/json',\n",
    "        accept='application/json',\n",
    "        body=json.dumps(request_body)\n",
    "    )\n",
    "    \n",
    "    response_body = json.loads(response['body'].read())\n",
    "    \n",
    "    # Display metrics\n",
    "    display_metrics(response_body, f\"Section Analysis (pages {start_page+1}-{end_page})\")\n",
    "    \n",
    "    return response_body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_qa_system(pdf_path):\n",
    "    \"\"\"Create a Q&A system for a document with metrics tracking\"\"\"\n",
    "    \n",
    "    def ask_question(question):\n",
    "        response = analyze_pdf_with_citations(pdf_path, question)\n",
    "        display_response_with_citations(response)\n",
    "        return response\n",
    "    \n",
    "    return ask_question\n",
    "\n",
    "def analyze_research_paper(pdf_path):\n",
    "    \"\"\"Analyze a research paper with predefined questions and cumulative metrics\"\"\"\n",
    "    \n",
    "    questions = [\n",
    "        \"What is the main research question or hypothesis?\",\n",
    "        \"What methodology was used in this research?\",\n",
    "        \"What are the key findings or results?\",\n",
    "        \"What limitations are mentioned?\",\n",
    "        \"What are the future research directions suggested?\"\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    cumulative_metrics = {\n",
    "        'total_input_tokens': 0,\n",
    "        'total_output_tokens': 0,\n",
    "        'total_cache_creation_tokens': 0,\n",
    "        'total_cache_read_tokens': 0,\n",
    "        'calls_with_cache_hits': 0\n",
    "    }\n",
    "    \n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Question {i}/{len(questions)}: {question}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        response = analyze_pdf_with_citations(pdf_path, question)\n",
    "        display_response_with_citations(response)\n",
    "        \n",
    "        # Update cumulative metrics\n",
    "        if response:\n",
    "            usage = response.get('usage', {})\n",
    "            cumulative_metrics['total_input_tokens'] += usage.get('input_tokens', 0)\n",
    "            cumulative_metrics['total_output_tokens'] += usage.get('output_tokens', 0)\n",
    "            cumulative_metrics['total_cache_creation_tokens'] += usage.get('cache_creation_input_tokens', 0)\n",
    "            cumulative_metrics['total_cache_read_tokens'] += usage.get('cache_read_input_tokens', 0)\n",
    "            \n",
    "            if usage.get('cache_read_input_tokens', 0) > 0:\n",
    "                cumulative_metrics['calls_with_cache_hits'] += 1\n",
    "        \n",
    "        results[question] = response\n",
    "    \n",
    "    # Display cumulative metrics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CUMULATIVE METRICS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total API calls: {len(questions)}\")\n",
    "    print(f\"Total input tokens: {cumulative_metrics['total_input_tokens']:,}\")\n",
    "    print(f\"Total output tokens: {cumulative_metrics['total_output_tokens']:,}\")\n",
    "    print(f\"Total tokens: {cumulative_metrics['total_input_tokens'] + cumulative_metrics['total_output_tokens']:,}\")\n",
    "    \n",
    "    if cumulative_metrics['total_cache_creation_tokens'] > 0:\n",
    "        print(f\"\\nPrompt Caching Statistics:\")\n",
    "        print(f\"  Cache creation tokens: {cumulative_metrics['total_cache_creation_tokens']:,}\")\n",
    "        print(f\"  Cache read tokens: {cumulative_metrics['total_cache_read_tokens']:,}\")\n",
    "        print(f\"  Calls with cache hits: {cumulative_metrics['calls_with_cache_hits']}/{len(questions)}\")\n",
    "        \n",
    "        if cumulative_metrics['total_cache_read_tokens'] > 0:\n",
    "            cache_efficiency = (cumulative_metrics['total_cache_read_tokens'] / \n",
    "                              cumulative_metrics['total_input_tokens'] * 100)\n",
    "            print(f\"  Overall cache efficiency: {cache_efficiency:.1f}%\")\n",
    "            \n",
    "            # Calculate token savings\n",
    "            tokens_saved = cumulative_metrics['total_cache_read_tokens'] * 0.9  # 90% discount on cached tokens\n",
    "            print(f\"  Estimated tokens saved: {int(tokens_saved):,}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Basic PDF Analysis\n",
    "\n",
    "Let's start with a simple question about the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing the PDF without citations...\n",
      "\n",
      "PDF Analysis Metrics:\n",
      "  Input tokens: 5635\n",
      "  Output tokens: 188\n",
      "  Total tokens: 5823\n",
      "The main topic of this document is **Advanced Language Models: Performance Analysis and Implementation Guidelines**. \n",
      "\n",
      "This technical report provides a comprehensive analysis of state-of-the-art language models, specifically focusing on:\n",
      "\n",
      "1. **Performance evaluation** - Comparing four leading language models (GPT-3.5, Claude 2, Claude 3, and GPT-4) across 50 diverse tasks\n",
      "2. **Implementation best practices** - Guidelines for deploying these models in production environments\n",
      "3. **Practical applications** - How organizations can effectively utilize these technologies\n",
      "\n",
      "The report presents key findings such as Claude 3's superior performance in complex reasoning tasks (92% accuracy), cost optimization strategies that can reduce expenses by up to 40%, and the benefits of proper prompt engineering. It's aimed at helping organizations understand how to select, implement, and optimize language models for various business applications.\n"
     ]
    }
   ],
   "source": [
    "# Basic analysis without citations\n",
    "print(\"Analyzing the PDF without citations...\")\n",
    "response = analyze_pdf(pdf_path, \"What is the main topic of this document?\")\n",
    "\n",
    "if response:\n",
    "    for block in response.get('content', []):\n",
    "        if block.get('type') == 'text':\n",
    "            print(block.get('text', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Extracting Key Findings with Citations\n",
    "\n",
    "Now let's use the citation feature to get precise references:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting key findings with citations...\n",
      "\n",
      "Citations Analysis Metrics:\n",
      "  Input tokens: 6807\n",
      "  Output tokens: 253\n",
      "  Total tokens: 7060\n",
      "Response:\n",
      "Based on the technical report, the key findings are:\n",
      "\n",
      "• Claude 3 demonstrates superior performance in complex reasoning tasks with 92% accuracy\n",
      "• Token optimization strategies can reduce costs by up to 40% without compromising quality\n",
      "• Hybrid approaches combining multiple models yield best results for enterprise applications\n",
      "• Proper prompt engineering increases task success rates by an average of 25%\n",
      "\n",
      "Additionally, the report highlights that modern language models show a 15% improvement in accuracy metrics over previous generations and a 2.3x increase in processing efficiency when properly optimized.\n",
      "\n",
      "The performance analysis also reveals specific comparative metrics, showing that Claude 3 achieved 92% accuracy with 140ms latency, outperforming other models like GPT-4 (91% accuracy, 150ms latency), Claude 2 (88% accuracy, 135ms latency), and GPT-3.5 (85% accuracy, 120ms latency).\n",
      "\n",
      "==================================================\n",
      "\n",
      "Citations:\n",
      "\n",
      "[1] \"Key Findings\n",
      "• Claude 3 demonstrates superior performance in complex reasoning tasks with 92% accuracy\n",
      "• Token optimization strategies can reduce costs by up to 40% without compromising quality\n",
      "• Hybrid approaches combining multiple models yield best results for enterprise applications\n",
      "• Proper prompt engineering increases task success rates by an average of 25%\n",
      "Page 1\"\n",
      "    Source: Technical Report\n",
      "    Pages: 1-2\n",
      "\n",
      "[2] \"Key findings include a 15% improvement in accuracy metrics over previous generations\n",
      "and a 2.3x increase in processing efficiency when properly optimized. \"\n",
      "    Source: Technical Report\n",
      "    Pages: 1-2\n",
      "\n",
      "[3] \"Table 1: Model Performance Metrics\n",
      "Model Accuracy (%) Latency (ms) Tokens/sec Cost Efficiency\n",
      "GPT-3.5 85 120 2,500 High\n",
      "Claude 2 88 135 2,200 Medium\n",
      "Claude 3 92 140 2,100 Medium\n",
      "GPT-4 91 150 1,800 Low\n",
      "Page 2\"\n",
      "    Source: Technical Report\n",
      "    Pages: 2-3\n"
     ]
    }
   ],
   "source": [
    "# Analysis with citations\n",
    "print(\"Extracting key findings with citations...\")\n",
    "response = analyze_pdf_with_citations(pdf_path, \"What are the key findings mentioned in this report?\")\n",
    "display_response_with_citations(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Understanding Visual Content\n",
    "\n",
    "PDFs often contain charts and graphs. Let's ask about the visual content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing charts and visual data...\n",
      "\n",
      "Citations Analysis Metrics:\n",
      "  Input tokens: 6817\n",
      "  Output tokens: 425\n",
      "  Total tokens: 7242\n",
      "Response:\n",
      "The document contains two charts that provide insights into language model performance:\n",
      "\n",
      "## 1. Model Performance Comparison Chart\n",
      "\n",
      "This is a bar chart comparing four language models (GPT-3.5, Claude 2, Claude 3, and GPT-4) across two metrics:\n",
      "- **Accuracy %** (shown in blue bars)\n",
      "- **Speed Score** (shown in purple bars)\n",
      "\n",
      "**Key insights:**\n",
      "- Claude 3 demonstrates the highest accuracy at 92%, followed closely by GPT-4 at 91%\n",
      "- GPT-3.5 shows the lowest accuracy at 85% but appears to have competitive speed performance\n",
      "- Claude 2 achieves 88% accuracy with medium performance across other metrics\n",
      "- The chart visually demonstrates the trade-offs between accuracy and speed across different models\n",
      "\n",
      "## 2. Token Usage Pattern Chart\n",
      "\n",
      "This is a line graph showing token usage over a 30-day period, with:\n",
      "- X-axis: Days of the month (0-30)\n",
      "- Y-axis: Tokens in thousands (0-1500)\n",
      "\n",
      "**Key insights:**\n",
      "- Token usage starts high (around 1,000-1,500 tokens) in the first 10 days\n",
      "- There's a significant decline in usage from days 10-20, dropping to around 500 tokens\n",
      "- Usage remains relatively low through day 25, then shows an uptick toward month-end\n",
      "- This pattern could indicate cyclical business usage, budget constraints, or optimization efforts taking effect mid-month\n",
      "\n",
      "These visualizations complement the detailed performance metrics table and support the report's findings about 15% improvement in accuracy metrics and 2.3x increase in processing efficiency when properly optimized.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Citations:\n",
      "\n",
      "[1] \"Key Findings\n",
      "• Claude 3 demonstrates superior performance in complex reasoning tasks with 92% accuracy\n",
      "• Token optimization strategies can reduce costs by up to 40% without compromising quality\n",
      "• Hybrid approaches combining multiple models yield best results for enterprise applications\n",
      "• Proper prompt engineering increases task success rates by an average of 25%\n",
      "Page 1\"\n",
      "    Source: Technical Report\n",
      "    Pages: 1-2\n",
      "\n",
      "[2] \"Table 1: Model Performance Metrics\n",
      "Model Accuracy (%) Latency (ms) Tokens/sec Cost Efficiency\n",
      "GPT-3.5 85 120 2,500 High\n",
      "Claude 2 88 135 2,200 Medium\n",
      "Claude 3 92 140 2,100 Medium\n",
      "GPT-4 91 150 1,800 Low\n",
      "Page 2\"\n",
      "    Source: Technical Report\n",
      "    Pages: 2-3\n",
      "\n",
      "[3] \"Table 1: Model Performance Metrics\n",
      "Model Accuracy (%) Latency (ms) Tokens/sec Cost Efficiency\n",
      "GPT-3.5 85 120 2,500 High\n",
      "Claude 2 88 135 2,200 Medium\n",
      "Claude 3 92 140 2,100 Medium\n",
      "GPT-4 91 150 1,800 Low\n",
      "Page 2\"\n",
      "    Source: Technical Report\n",
      "    Pages: 2-3\n",
      "\n",
      "[4] \"Key findings include a 15% improvement in accuracy metrics over previous generations\n",
      "and a 2.3x increase in processing efficiency when properly optimized. \"\n",
      "    Source: Technical Report\n",
      "    Pages: 1-2\n"
     ]
    }
   ],
   "source": [
    "# Analyzing visual content\n",
    "print(\"Analyzing charts and visual data...\")\n",
    "response = analyze_pdf_with_citations(\n",
    "    pdf_path, \n",
    "    \"Describe any charts or graphs in the document and what they show. What insights do they provide?\"\n",
    ")\n",
    "display_response_with_citations(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Analyzing Tables\n",
    "\n",
    "Let's extract and understand table data from the PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting and analyzing table information...\n",
      "Response:\n",
      "Based on my review of the document, there is one table present:\n",
      "\n",
      "**Table 1: Model Performance Metrics** (found on page 2)\n",
      "\n",
      "Here is the extracted table:\n",
      "\n",
      "| Model    | Accuracy (%) | Latency (ms) | Tokens/sec | Cost Efficiency |\n",
      "|----------|--------------|--------------|------------|-----------------|\n",
      "| GPT-3.5  | 85           | 120          | 2,500      | High           |\n",
      "| Claude 2 | 88           | 135          | 2,200      | Medium         |\n",
      "| Claude 3 | 92           | 140          | 2,100      | Medium         |\n",
      "| GPT-4    | 91           | 150          | 1,800      | Low            |\n",
      "\n",
      "## What the table shows:\n",
      "\n",
      "This table presents a comparative analysis of four leading language models across four key performance dimensions:\n",
      "\n",
      "1. **Accuracy (%)**: Claude 3 demonstrates superior performance with 92% accuracy, followed by GPT-4 (91%), Claude 2 (88%), and GPT-3.5 (85%).\n",
      "\n",
      "2. **Latency (ms)**: Response time measurements, with GPT-3.5 being fastest at 120ms and GPT-4 being slowest at 150ms.\n",
      "\n",
      "3. **Tokens/sec**: Throughput measurements showing GPT-3.5 has the highest processing speed at 2,500 tokens per second, while GPT-4 has the lowest at 1,800 tokens per second.\n",
      "\n",
      "4. **Cost Efficiency**: A qualitative assessment where GPT-3.5 offers \"High\" cost efficiency, Claude 2 and 3 offer \"Medium\" efficiency, and GPT-4 has \"Low\" cost efficiency.\n",
      "\n",
      "The table reveals trade-offs between accuracy, speed, and cost, helping users select the most appropriate model based on their specific requirements and constraints.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Citations:\n",
      "\n",
      "[1] \"Table 1: Model Performance Metrics\n",
      "Model Accuracy (%) Latency (ms) Tokens/sec Cost Efficiency\n",
      "GPT-3.5 85 120 2,500 High\n",
      "Claude 2 88 135 2,200 Medium\n",
      "Claude 3 92 140 2,100 Medium\n",
      "GPT-4 91 150 1,800 Low\n",
      "Page 2\"\n",
      "    Source: Research Document\n",
      "    Pages: 2-3\n",
      "\n",
      "[2] \"Key Findings\n",
      "• Claude 3 demonstrates superior performance in complex reasoning tasks with 92% accuracy\n",
      "• Token optimization strategies can reduce costs by up to 40% without compromising quality\n",
      "• Hybrid approaches combining multiple models yield best results for enterprise applications\n",
      "• Proper prompt engineering increases task success rates by an average of 25%\n",
      "Page 1\"\n",
      "    Source: Research Document\n",
      "    Pages: 1-2\n"
     ]
    }
   ],
   "source": [
    "# Analyzing table data\n",
    "print(\"Extracting and analyzing table information...\")\n",
    "response = analyze_pdf_with_citations(\n",
    "    pdf_path, \n",
    "    \"What tables are in this document? Extract the performance metrics table and explain what it shows.\"\n",
    ")\n",
    "display_response_with_citations(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: Creating a Document Q&A System\n",
    "\n",
    "Let's create an interactive Q&A system for our technical report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Question: What are the implementation guidelines mentioned?\n",
      "============================================================\n",
      "\n",
      "Response:\n",
      "Based on the technical report, here are the implementation guidelines mentioned for deploying language models in production environments:\n",
      "\n",
      "## Model Selection\n",
      "Choose models based on specific use case requirements. Claude 3 excels at complex reasoning, while GPT-3.5 offers optimal speed for simple tasks.\n",
      "\n",
      "## Prompt Engineering\n",
      "Invest in developing clear, structured prompts. Include examples and explicit instructions to improve output quality.\n",
      "\n",
      "## Cost Optimization\n",
      "Implement token counting and caching strategies. Use smaller models for initial filtering before engaging larger models.\n",
      "\n",
      "## Quality Assurance\n",
      "Establish automated testing pipelines with diverse test cases. Monitor model outputs for drift and degradation.\n",
      "\n",
      "## Security Considerations\n",
      "Implement proper input sanitization and output filtering. Never expose raw model outputs without validation.\n",
      "\n",
      "The report emphasizes that with proper implementation strategies, these models can deliver substantial value in automation, analysis, and decision support applications. The key to success lies in understanding model strengths, implementing robust engineering practices, and maintaining continuous evaluation processes.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Citations:\n",
      "\n",
      "[1] \"Technical Report TR-2024-001\n",
      "Implementation Guidelines\n",
      "Based on our extensive analysis, we recommend the following best practices for implementing language\n",
      "models in production environments:\n",
      "Model Selection:\n",
      "Choose models based on specific use case requirements. Claude 3 excels at complex reasoning, while\n",
      "GPT-3.5 offers optimal speed for simple tasks.\n",
      "\"\n",
      "    Source: Research Document\n",
      "    Pages: 3-4\n",
      "\n",
      "[2] \"Prompt Engineering:\n",
      "Invest in developing clear, structured prompts. Include examples and explicit instructions to improve output\n",
      "quality.\n",
      "\"\n",
      "    Source: Research Document\n",
      "    Pages: 3-4\n",
      "\n",
      "[3] \"Cost Optimization:\n",
      "Implement token counting and caching strategies. Use smaller models for initial filtering before engaging\n",
      "larger models.\n",
      "\"\n",
      "    Source: Research Document\n",
      "    Pages: 3-4\n",
      "\n",
      "[4] \"Quality Assurance:\n",
      "Establish automated testing pipelines with diverse test cases. Monitor model outputs for drift and\n",
      "degradation.\n",
      "\"\n",
      "    Source: Research Document\n",
      "    Pages: 3-4\n",
      "\n",
      "[5] \"Security Considerations:\n",
      "Implement proper input sanitization and output filtering. Never expose raw model outputs without validation.\n",
      "\"\n",
      "    Source: Research Document\n",
      "    Pages: 3-4\n",
      "\n",
      "[6] \"Our analysis demonstrates that with proper implementation strategies, these models can\n",
      "deliver substantial value in automation, analysis, and decision support applications. The key to success lies in\n",
      "understanding model strengths, implementing robust engineering practices, and maintaining continuous\n",
      "evaluation processes.\n",
      "\"\n",
      "    Source: Research Document\n",
      "    Pages: 3-4\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "Question: What models are compared in this report?\n",
      "============================================================\n",
      "\n",
      "Response:\n",
      "Based on the research document, the report compares four leading language models:\n",
      "\n",
      "1. GPT-3.5\n",
      "2. Claude 2  \n",
      "3. Claude 3\n",
      "4. GPT-4\n",
      "\n",
      "The comprehensive evaluation methodology involved testing these four leading language models across 50 diverse tasks spanning natural language understanding, generation, and reasoning capabilities. The comparison includes performance metrics such as accuracy percentages, latency in milliseconds, tokens per second, and cost efficiency for each model.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Citations:\n",
      "\n",
      "[1] \"Table 1: Model Performance Metrics\n",
      "Model Accuracy (%) Latency (ms) Tokens/sec Cost Efficiency\n",
      "GPT-3.5 85 120 2,500 High\n",
      "Claude 2 88 135 2,200 Medium\n",
      "Claude 3 92 140 2,100 Medium\n",
      "GPT-4 91 150 1,800 Low\n",
      "Page 2\"\n",
      "    Source: Research Document\n",
      "    Pages: 2-3\n",
      "\n",
      "[2] \"Technical Report TR-2024-001\n",
      "Performance Analysis\n",
      "Our comprehensive evaluation methodology involved testing four leading language models across 50 diverse\n",
      "tasks spanning natural language understanding, generation, and reasoning capabilities. \"\n",
      "    Source: Research Document\n",
      "    Pages: 2-3\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "Question: What are the conclusions about language model capabilities?\n",
      "============================================================\n",
      "\n",
      "Response:\n",
      "Based on the research document, the conclusions about language model capabilities are quite positive and forward-looking:\n",
      "\n",
      "The rapid advancement in language model capabilities presents significant opportunities for organizations across all sectors. Our analysis demonstrates that with proper implementation strategies, these models can deliver substantial value in automation, analysis, and decision support applications.\n",
      "\n",
      "The report emphasizes that success with these advanced capabilities depends on three key factors: The key to success lies in understanding model strengths, implementing robust engineering practices, and maintaining continuous evaluation processes.\n",
      "\n",
      "The document also highlights some specific performance achievements that demonstrate these enhanced capabilities:\n",
      "\n",
      "- Our research indicates that modern language models have achieved remarkable capabilities in natural language understanding and generation tasks. Key findings include a 15% improvement in accuracy metrics over previous generations and a 2.3x increase in processing efficiency when properly optimized.\n",
      "\n",
      "- Claude 3 demonstrates superior performance in complex reasoning tasks with 92% accuracy\n",
      "\n",
      "- Proper prompt engineering increases task success rates by an average of 25%\n",
      "\n",
      "Overall, the conclusions suggest that language models have reached a level of sophistication where they can provide significant business value across various applications, but realizing this potential requires thoughtful implementation and ongoing management practices.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Citations:\n",
      "\n",
      "[1] \"Conclusions\n",
      "The rapid advancement in language model capabilities presents significant opportunities for organizations\n",
      "across all sectors. \"\n",
      "    Source: Research Document\n",
      "    Pages: 3-4\n",
      "\n",
      "[2] \"Our analysis demonstrates that with proper implementation strategies, these models can\n",
      "deliver substantial value in automation, analysis, and decision support applications. \"\n",
      "    Source: Research Document\n",
      "    Pages: 3-4\n",
      "\n",
      "[3] \"The key to success lies in\n",
      "understanding model strengths, implementing robust engineering practices, and maintaining continuous\n",
      "evaluation processes.\n",
      "\"\n",
      "    Source: Research Document\n",
      "    Pages: 3-4\n",
      "\n",
      "[4] \"Our research indicates that\n",
      "modern language models have achieved remarkable capabilities in natural language understanding and\n",
      "generation tasks. Key findings include a 15% improvement in accuracy metrics over previous generations\n",
      "and a 2.3x increase in processing efficiency when properly optimized. \"\n",
      "    Source: Research Document\n",
      "    Pages: 1-2\n",
      "\n",
      "[5] \"Key Findings\n",
      "• Claude 3 demonstrates superior performance in complex reasoning tasks with 92% accuracy\n",
      "• Token optimization strategies can reduce costs by up to 40% without compromising quality\n",
      "• Hybrid approaches combining multiple models yield best results for enterprise applications\n",
      "• Proper prompt engineering increases task success rates by an average of 25%\n",
      "Page 1\"\n",
      "    Source: Research Document\n",
      "    Pages: 1-2\n",
      "\n",
      "[6] \"Key Findings\n",
      "• Claude 3 demonstrates superior performance in complex reasoning tasks with 92% accuracy\n",
      "• Token optimization strategies can reduce costs by up to 40% without compromising quality\n",
      "• Hybrid approaches combining multiple models yield best results for enterprise applications\n",
      "• Proper prompt engineering increases task success rates by an average of 25%\n",
      "Page 1\"\n",
      "    Source: Research Document\n",
      "    Pages: 1-2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Q&A system for the technical report\n",
    "qa_system = create_document_qa_system(pdf_path)\n",
    "\n",
    "# Ask various questions\n",
    "questions = [\n",
    "    \"What are the implementation guidelines mentioned?\",\n",
    "    \"What models are compared in this report?\",\n",
    "    \"What are the conclusions about language model capabilities?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    qa_system(question)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6: Comprehensive Research Paper Analysis\n",
    "\n",
    "Let's perform a full analysis of the document as if it were a research paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing comprehensive document analysis...\n",
      "This will analyze the document from multiple perspectives...\n",
      "\n",
      "\n",
      "============================================================\n",
      "Question: What is the main research question or hypothesis?\n",
      "============================================================\n",
      "\n",
      "Response:\n",
      "Based on the technical report, the main research focus appears to be a comprehensive performance evaluation and practical implementation analysis of advanced language models, rather than testing a specific hypothesis. \n",
      "\n",
      "The research aims to address several key questions:\n",
      "\n",
      "1. **Performance Comparison**: How do leading language models perform across diverse tasks spanning natural language understanding, generation, and reasoning capabilities\n",
      "\n",
      "2. **Implementation Effectiveness**: How can organizations implement these technologies effectively, with the goal of providing actionable recommendations for organizations looking to implement these technologies effectively\n",
      "\n",
      "3. **Optimization Strategies**: How various optimization approaches (token optimization, prompt engineering, hybrid approaches) impact performance and cost-efficiency\n",
      "\n",
      "The research methodology involved testing four leading language models across 50 diverse tasks using both quantitative metrics (accuracy, latency, throughput) and qualitative assessments (coherence, relevance, factual accuracy).\n",
      "\n",
      "Rather than testing a single hypothesis, this appears to be an empirical study designed to provide practical guidance for implementing language models in production environments, with the overarching goal of demonstrating that with proper implementation strategies, these models can deliver substantial value in automation, analysis, and decision support applications.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Citations:\n",
      "\n",
      "[1] \"Technical Report TR-2024-001\n",
      "Performance Analysis\n",
      "Our comprehensive evaluation methodology involved testing four leading language models across 50 diverse\n",
      "tasks spanning natural language understanding, generation, and reasoning capabilities. \"\n",
      "    Source: Research Document\n",
      "    Pages: 2-3\n",
      "\n",
      "[2] \"Technical Report TR-2024-001\n",
      "Advanced Language Models:\n",
      "Performance Analysis and\n",
      "Implementation Guidelines\n",
      "AI Research Division\n",
      "Published: June 2025\n",
      "Executive Summary\n",
      "This technical report presents a comprehensive analysis of state-of-the-art language models, focusing on\n",
      "performance metrics, implementation best practices, and practical applications. \"\n",
      "    Source: Research Document\n",
      "    Pages: 1-2\n",
      "\n",
      "[3] \"This report provides actionable\n",
      "recommendations for organizations looking to implement these technologies effectively.\n",
      "\"\n",
      "    Source: Research Document\n",
      "    Pages: 1-2\n",
      "\n",
      "[4] \"This report provides actionable\n",
      "recommendations for organizations looking to implement these technologies effectively.\n",
      "\"\n",
      "    Source: Research Document\n",
      "    Pages: 1-2\n",
      "\n",
      "[5] \"Technical Report TR-2024-001\n",
      "Performance Analysis\n",
      "Our comprehensive evaluation methodology involved testing four leading language models across 50 diverse\n",
      "tasks spanning natural language understanding, generation, and reasoning capabilities. \"\n",
      "    Source: Research Document\n",
      "    Pages: 2-3\n",
      "\n",
      "[6] \"The evaluation\n",
      "framework incorporated both quantitative metrics (accuracy, latency, throughput) and qualitative\n",
      "assessments (coherence, relevance, factual accuracy).\n",
      "\"\n",
      "    Source: Research Document\n",
      "    Pages: 2-3\n",
      "\n",
      "[7] \"Our analysis demonstrates that with proper implementation strategies, these models can\n",
      "deliver substantial value in automation, analysis, and decision support applications. \"\n",
      "    Source: Research Document\n",
      "    Pages: 3-4\n",
      "\n",
      "============================================================\n",
      "Question: What methodology was used in this research?\n",
      "============================================================\n",
      "\n",
      "Response:\n",
      "Based on the research document, the methodology employed in this study involved several key components:\n",
      "\n",
      "## Testing Framework\n",
      "The comprehensive evaluation methodology involved testing four leading language models across 50 diverse tasks spanning natural language understanding, generation, and reasoning capabilities. The evaluation framework incorporated both quantitative metrics (accuracy, latency, throughput) and qualitative assessments (coherence, relevance, factual accuracy).\n",
      "\n",
      "## Sampling Approach\n",
      "The testing framework employed a stratified sampling approach across multiple domains: scientific literature, technical documentation, creative writing, and conversational interactions.\n",
      "\n",
      "## Standardized Testing Conditions\n",
      "Each model was evaluated using identical prompts and scoring criteria. Response times were measured under controlled conditions with consistent hardware specifications.\n",
      "\n",
      "## Technical Configuration\n",
      "All models were tested using the following configuration: AWS EC2 p3.2xlarge instances with NVIDIA V100 GPUs, 16GB GPU memory, and optimized inference libraries. API calls were made through AWS Bedrock with standard throttling limits applied.\n",
      "\n",
      "This methodology ensured a comprehensive and fair comparison of the four language models (GPT-3.5, Claude 2, Claude 3, and GPT-4) by maintaining consistent testing conditions while evaluating performance across diverse use cases and domains.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Citations:\n",
      "\n",
      "[1] \"Technical Report TR-2024-001\n",
      "Performance Analysis\n",
      "Our comprehensive evaluation methodology involved testing four leading language models across 50 diverse\n",
      "tasks spanning natural language understanding, generation, and reasoning capabilities. \"\n",
      "    Source: Research Document\n",
      "    Pages: 2-3\n",
      "\n",
      "[2] \"The evaluation\n",
      "framework incorporated both quantitative metrics (accuracy, latency, throughput) and qualitative\n",
      "assessments (coherence, relevance, factual accuracy).\n",
      "\"\n",
      "    Source: Research Document\n",
      "    Pages: 2-3\n",
      "\n",
      "[3] \"Methodology\n",
      "The testing framework employed a stratified sampling approach across multiple domains: scientific literature,\n",
      "technical documentation, creative writing, and conversational interactions. \"\n",
      "    Source: Research Document\n",
      "    Pages: 2-3\n",
      "\n",
      "[4] \"Each model was evaluated using\n",
      "identical prompts and scoring criteria. \"\n",
      "    Source: Research Document\n",
      "    Pages: 2-3\n",
      "\n",
      "[5] \"Response times were measured under controlled conditions with\n",
      "consistent hardware specifications.\n",
      "\"\n",
      "    Source: Research Document\n",
      "    Pages: 2-3\n",
      "\n",
      "[6] \"Technical Specifications\n",
      "All models were tested using the following configuration: AWS EC2 p3.2xlarge instances with NVIDIA V100\n",
      "GPUs, 16GB GPU memory, and optimized inference libraries. \"\n",
      "    Source: Research Document\n",
      "    Pages: 3-4\n",
      "\n",
      "[7] \"API calls were made through AWS Bedrock\n",
      "with standard throttling limits applied.\n",
      "\"\n",
      "    Source: Research Document\n",
      "    Pages: 3-4\n",
      "\n",
      "============================================================\n",
      "Question: What are the key findings or results?\n",
      "============================================================\n",
      "\n",
      "Response:\n",
      "Based on the research document, here are the key findings and results:\n",
      "\n",
      "## Performance Improvements\n",
      "Modern language models showed a 15% improvement in accuracy metrics over previous generations and a 2.3x increase in processing efficiency when properly optimized.\n",
      "\n",
      "## Model Performance Rankings\n",
      "Claude 3 demonstrates superior performance in complex reasoning tasks with 92% accuracy, making it the top performer among the tested models. The detailed performance metrics show:\n",
      "- Claude 3: 92% accuracy, 140ms latency, 2,100 tokens/sec, medium cost efficiency\n",
      "- GPT-4: 91% accuracy, 150ms latency, 1,800 tokens/sec, low cost efficiency\n",
      "- Claude 2: 88% accuracy, 135ms latency, 2,200 tokens/sec, medium cost efficiency\n",
      "- GPT-3.5: 85% accuracy, 120ms latency, 2,500 tokens/sec, high cost efficiency\n",
      "\n",
      "## Cost and Optimization Benefits\n",
      "Token optimization strategies can reduce costs by up to 40% without compromising quality, providing significant economic advantages for implementation.\n",
      "\n",
      "## Implementation Strategy Insights\n",
      "Hybrid approaches combining multiple models yield best results for enterprise applications, suggesting that using different models for different tasks is more effective than relying on a single model.\n",
      "\n",
      "## Prompt Engineering Impact\n",
      "Proper prompt engineering increases task success rates by an average of 25%, highlighting the importance of well-crafted prompts for optimal performance.\n",
      "\n",
      "## Testing Methodology\n",
      "The evaluation involved testing four leading language models across 50 diverse tasks spanning natural language understanding, generation, and reasoning capabilities, incorporating both quantitative metrics (accuracy, latency, throughput) and qualitative assessments (coherence, relevance, factual accuracy).\n",
      "\n",
      "==================================================\n",
      "\n",
      "Citations:\n",
      "\n",
      "[1] \"Key findings include a 15% improvement in accuracy metrics over previous generations\n",
      "and a 2.3x increase in processing efficiency when properly optimized. \"\n",
      "    Source: Research Document\n",
      "    Pages: 1-2\n",
      "\n",
      "[2] \"Key Findings\n",
      "• Claude 3 demonstrates superior performance in complex reasoning tasks with 92% accuracy\n",
      "• Token optimization strategies can reduce costs by up to 40% without compromising quality\n",
      "• Hybrid approaches combining multiple models yield best results for enterprise applications\n",
      "• Proper prompt engineering increases task success rates by an average of 25%\n",
      "Page 1\"\n",
      "    Source: Research Document\n",
      "    Pages: 1-2\n",
      "\n",
      "[3] \"Table 1: Model Performance Metrics\n",
      "Model Accuracy (%) Latency (ms) Tokens/sec Cost Efficiency\n",
      "GPT-3.5 85 120 2,500 High\n",
      "Claude 2 88 135 2,200 Medium\n",
      "Claude 3 92 140 2,100 Medium\n",
      "GPT-4 91 150 1,800 Low\n",
      "Page 2\"\n",
      "    Source: Research Document\n",
      "    Pages: 2-3\n",
      "\n",
      "[4] \"Table 1: Model Performance Metrics\n",
      "Model Accuracy (%) Latency (ms) Tokens/sec Cost Efficiency\n",
      "GPT-3.5 85 120 2,500 High\n",
      "Claude 2 88 135 2,200 Medium\n",
      "Claude 3 92 140 2,100 Medium\n",
      "GPT-4 91 150 1,800 Low\n",
      "Page 2\"\n",
      "    Source: Research Document\n",
      "    Pages: 2-3\n",
      "\n",
      "[5] \"Table 1: Model Performance Metrics\n",
      "Model Accuracy (%) Latency (ms) Tokens/sec Cost Efficiency\n",
      "GPT-3.5 85 120 2,500 High\n",
      "Claude 2 88 135 2,200 Medium\n",
      "Claude 3 92 140 2,100 Medium\n",
      "GPT-4 91 150 1,800 Low\n",
      "Page 2\"\n",
      "    Source: Research Document\n",
      "    Pages: 2-3\n",
      "\n",
      "[6] \"Table 1: Model Performance Metrics\n",
      "Model Accuracy (%) Latency (ms) Tokens/sec Cost Efficiency\n",
      "GPT-3.5 85 120 2,500 High\n",
      "Claude 2 88 135 2,200 Medium\n",
      "Claude 3 92 140 2,100 Medium\n",
      "GPT-4 91 150 1,800 Low\n",
      "Page 2\"\n",
      "    Source: Research Document\n",
      "    Pages: 2-3\n",
      "\n",
      "[7] \"Key Findings\n",
      "• Claude 3 demonstrates superior performance in complex reasoning tasks with 92% accuracy\n",
      "• Token optimization strategies can reduce costs by up to 40% without compromising quality\n",
      "• Hybrid approaches combining multiple models yield best results for enterprise applications\n",
      "• Proper prompt engineering increases task success rates by an average of 25%\n",
      "Page 1\"\n",
      "    Source: Research Document\n",
      "    Pages: 1-2\n",
      "\n",
      "[8] \"Key Findings\n",
      "• Claude 3 demonstrates superior performance in complex reasoning tasks with 92% accuracy\n",
      "• Token optimization strategies can reduce costs by up to 40% without compromising quality\n",
      "• Hybrid approaches combining multiple models yield best results for enterprise applications\n",
      "• Proper prompt engineering increases task success rates by an average of 25%\n",
      "Page 1\"\n",
      "    Source: Research Document\n",
      "    Pages: 1-2\n",
      "\n",
      "[9] \"Key Findings\n",
      "• Claude 3 demonstrates superior performance in complex reasoning tasks with 92% accuracy\n",
      "• Token optimization strategies can reduce costs by up to 40% without compromising quality\n",
      "• Hybrid approaches combining multiple models yield best results for enterprise applications\n",
      "• Proper prompt engineering increases task success rates by an average of 25%\n",
      "Page 1\"\n",
      "    Source: Research Document\n",
      "    Pages: 1-2\n",
      "\n",
      "[10] \"Technical Report TR-2024-001\n",
      "Performance Analysis\n",
      "Our comprehensive evaluation methodology involved testing four leading language models across 50 diverse\n",
      "tasks spanning natural language understanding, generation, and reasoning capabilities. The evaluation\n",
      "framework incorporated both quantitative metrics (accuracy, latency, throughput) and qualitative\n",
      "assessments (coherence, relevance, factual accuracy).\n",
      "\"\n",
      "    Source: Research Document\n",
      "    Pages: 2-3\n",
      "\n",
      "============================================================\n",
      "Question: What limitations are mentioned?\n",
      "============================================================\n",
      "\n",
      "Response:\n",
      "Based on the research document, while it doesn't explicitly list limitations in a dedicated section, several limitations can be inferred from the content:\n",
      "\n",
      "## Performance Trade-offs\n",
      "\n",
      "The models show varying performance across different metrics, with no single model excelling in all areas. For example, GPT-4 has low cost efficiency despite high accuracy (91%), while Claude 3 achieves the highest accuracy (92%) but with medium cost efficiency.\n",
      "\n",
      "## Cost Considerations\n",
      "\n",
      "The document indicates that cost optimization is necessary, noting that \"Token optimization strategies can reduce costs by up to 40% without compromising quality,\" suggesting that without proper optimization, costs can be prohibitively high.\n",
      "\n",
      "## Implementation Complexity\n",
      "\n",
      "The document implies several implementation challenges:\n",
      "\n",
      "- The need for significant investment in prompt engineering: \"Invest in developing clear, structured prompts. Include examples and explicit instructions to improve output quality.\"\n",
      "\n",
      "- Quality assurance requirements: \"Establish automated testing pipelines with diverse test cases. Monitor model outputs for drift and degradation.\"\n",
      "\n",
      "- Security vulnerabilities: \"Implement proper input sanitization and output filtering. Never expose raw model outputs without validation.\"\n",
      "\n",
      "## Model-Specific Limitations\n",
      "\n",
      "The document suggests that models have specific use case constraints: \"Claude 3 excels at complex reasoning, while GPT-3.5 offers optimal speed for simple tasks,\" indicating that no single model is universally optimal for all applications.\n",
      "\n",
      "The document emphasizes that success requires \"understanding model strengths, implementing robust engineering practices, and maintaining continuous evaluation processes,\" which implies significant ongoing effort and expertise requirements.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Citations:\n",
      "\n",
      "[1] \"Table 1: Model Performance Metrics\n",
      "Model Accuracy (%) Latency (ms) Tokens/sec Cost Efficiency\n",
      "GPT-3.5 85 120 2,500 High\n",
      "Claude 2 88 135 2,200 Medium\n",
      "Claude 3 92 140 2,100 Medium\n",
      "GPT-4 91 150 1,800 Low\n",
      "Page 2\"\n",
      "    Source: Research Document\n",
      "    Pages: 2-3\n",
      "\n",
      "[2] \"Key Findings\n",
      "• Claude 3 demonstrates superior performance in complex reasoning tasks with 92% accuracy\n",
      "• Token optimization strategies can reduce costs by up to 40% without compromising quality\n",
      "• Hybrid approaches combining multiple models yield best results for enterprise applications\n",
      "• Proper prompt engineering increases task success rates by an average of 25%\n",
      "Page 1\"\n",
      "    Source: Research Document\n",
      "    Pages: 1-2\n",
      "\n",
      "[3] \"Prompt Engineering:\n",
      "Invest in developing clear, structured prompts. Include examples and explicit instructions to improve output\n",
      "quality.\n",
      "\"\n",
      "    Source: Research Document\n",
      "    Pages: 3-4\n",
      "\n",
      "[4] \"Quality Assurance:\n",
      "Establish automated testing pipelines with diverse test cases. Monitor model outputs for drift and\n",
      "degradation.\n",
      "\"\n",
      "    Source: Research Document\n",
      "    Pages: 3-4\n",
      "\n",
      "[5] \"Security Considerations:\n",
      "Implement proper input sanitization and output filtering. Never expose raw model outputs without validation.\n",
      "\"\n",
      "    Source: Research Document\n",
      "    Pages: 3-4\n",
      "\n",
      "[6] \"Claude 3 excels at complex reasoning, while\n",
      "GPT-3.5 offers optimal speed for simple tasks.\n",
      "\"\n",
      "    Source: Research Document\n",
      "    Pages: 3-4\n",
      "\n",
      "[7] \"The key to success lies in\n",
      "understanding model strengths, implementing robust engineering practices, and maintaining continuous\n",
      "evaluation processes.\n",
      "\"\n",
      "    Source: Research Document\n",
      "    Pages: 3-4\n",
      "\n",
      "============================================================\n",
      "Question: What are the future research directions suggested?\n",
      "============================================================\n",
      "\n",
      "Response:\n",
      "I've carefully reviewed the entire document, but it does not contain any explicit section or discussion about future research directions. The technical report focuses on:\n",
      "\n",
      "- Current performance analysis of existing language models\n",
      "- Implementation guidelines and best practices\n",
      "- Methodology and results from their evaluation\n",
      "- Conclusions about the current state of language model capabilities\n",
      "\n",
      "While the document concludes that the rapid advancement in language model capabilities presents significant opportunities for organizations across all sectors, and that with proper implementation strategies, these models can deliver substantial value in automation, analysis, and decision support applications, it does not specifically outline future research directions or areas for continued investigation.\n",
      "\n",
      "The report appears to be focused on practical implementation guidance rather than identifying gaps or proposing new research avenues. If you're looking for information about future research directions in language models, you may need to consult additional sources beyond this particular technical report.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Citations:\n",
      "\n",
      "[1] \"Conclusions\n",
      "The rapid advancement in language model capabilities presents significant opportunities for organizations\n",
      "across all sectors. Our analysis demonstrates that with proper implementation strategies, these models can\n",
      "deliver substantial value in automation, analysis, and decision support applications. \"\n",
      "    Source: Research Document\n",
      "    Pages: 3-4\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive analysis\n",
    "print(\"Performing comprehensive document analysis...\")\n",
    "print(\"This will analyze the document from multiple perspectives...\\n\")\n",
    "\n",
    "# Note: This might take a moment as it makes multiple API calls\n",
    "results = analyze_research_paper(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 7: Streaming Response for Better UX\n",
    "\n",
    "For long analyses, streaming provides immediate feedback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming analysis of the document...\n",
      "\n",
      "Summary: # Technical Report TR-2024-001: Advanced Language Models - Comprehensive Summary\n",
      "\n",
      "## Overview\n",
      "This comprehensive technical report from the AI Research Division (published June 2025) analyzes state-of-the-art language models, focusing on performance metrics, implementation strategies, and practical applications for organizations.\n",
      "\n",
      "## Executive Summary\n",
      "The report demonstrates that modern language models have achieved remarkable capabilities in natural language understanding and generation. Key improvements include:\n",
      "- **15% improvement** in accuracy metrics over previous generations\n",
      "- **2.3x increase** in processing efficiency when properly optimized\n",
      "- Actionable recommendations for effective organizational implementation\n",
      "\n",
      "## Key Research Findings\n",
      "\n",
      "### Performance Highlights\n",
      "- **Claude 3**: Superior performance in complex reasoning tasks with **92% accuracy**\n",
      "- **Token optimization**: Can reduce costs by up to **40%** without quality compromise\n",
      "- **Hybrid approaches**: Combining multiple models yields optimal results for enterprise applications\n",
      "- **Prompt engineering**: Increases task success rates by an average of **25%**\n",
      "\n",
      "## Performance Analysis\n",
      "\n",
      "### Methodology\n",
      "The evaluation framework tested four leading language models across 50 diverse tasks using:\n",
      "- **Quantitative metrics**: Accuracy, latency, throughput\n",
      "- **Qualitative assessments**: Coherence, relevance, factual accuracy\n",
      "- **Stratified sampling** across domains: scientific literature, technical documentation, creative writing, conversational interactions\n",
      "\n",
      "### Model Performance Comparison\n",
      "\n",
      "| Model | Accuracy (%) | Latency (ms) | Tokens/sec | Cost Efficiency |\n",
      "|-------|-------------|-------------|------------|-----------------|\n",
      "| GPT-3.5 | 85 | 120 | 2,500 | High |\n",
      "| Claude 2 | 88 | 135 | 2,200 | Medium |\n",
      "| **Claude 3** | **92** | 140 | 2,100 | Medium |\n",
      "| GPT-4 | 91 | 150 | 1,800 | Low |\n",
      "\n",
      "### Visual Analytics\n",
      "The report includes performance comparison charts and token usage patterns over a 30-day period, showing varying computational demands and efficiency trends.\n",
      "\n",
      "## Implementation Guidelines\n",
      "\n",
      "### Best Practices for Production Deployment\n",
      "\n",
      "**Model Selection:**\n",
      "- Choose based on specific use case requirements\n",
      "- Claude 3 for complex reasoning tasks\n",
      "- GPT-3.5 for speed-critical simple tasks\n",
      "\n",
      "**Prompt Engineering:**\n",
      "- Develop clear, structured prompts\n",
      "- Include examples and explicit instructions\n",
      "- Focus on improving output quality\n",
      "\n",
      "**Cost Optimization:**\n",
      "- Implement token counting and caching strategies\n",
      "- Use smaller models for initial filtering\n",
      "- Engage larger models only when necessary\n",
      "\n",
      "**Quality Assurance:**\n",
      "- Establish automated testing pipelines\n",
      "- Use diverse test cases\n",
      "- Monitor for model drift and degradation\n",
      "\n",
      "**Security Considerations:**\n",
      "- Implement proper input sanitization\n",
      "- Apply output filtering\n",
      "- Never expose raw model outputs without validation\n",
      "\n",
      "## Technical Specifications\n",
      "\n",
      "### Testing Environment\n",
      "- **Hardware**: AWS EC2 p3.2xlarge instances\n",
      "- **GPU**: NVIDIA V100 GPUs with 16GB memory\n",
      "- **Platform**: AWS Bedrock with standard throttling\n",
      "- **Libraries**: Optimized inference libraries\n",
      "\n",
      "## Conclusions\n",
      "\n",
      "The report concludes that rapid advancement in language model capabilities presents significant opportunities across all sectors. Success factors include:\n",
      "\n",
      "1. **Understanding model strengths** and limitations\n",
      "2. **Implementing robust engineering practices**\n",
      "3. **Maintaining continuous evaluation processes**\n",
      "4. **Proper implementation strategies** for substantial value delivery in automation, analysis, and decision support\n",
      "\n",
      "## References\n",
      "The report cites key foundational research including:\n",
      "- Brown et al. (2020) - Few-Shot Learning\n",
      "- Anthropic (2024) - Claude 3 Technical Report\n",
      "- OpenAI (2023) - GPT-4 Technical Report\n",
      "- Zhang et al. (2024) - LLM Inference Optimization\n",
      "\n",
      "This technical report serves as a comprehensive guide for organizations considering advanced language model implementation, providing both strategic insights and practical implementation frameworks.\n",
      "\n",
      "Streaming Metrics:\n",
      "  Input tokens: 5641\n",
      "  Output tokens: 894\n",
      "  Total tokens: 6535\n",
      "\n",
      "\n",
      "Streaming complete!\n"
     ]
    }
   ],
   "source": [
    "# Streaming analysis for real-time feedback\n",
    "print(\"Streaming analysis of the document...\\n\")\n",
    "print(\"Summary: \", end=\"\")\n",
    "summary = stream_pdf_analysis(\n",
    "    pdf_path, \n",
    "    \"Provide a comprehensive summary of this technical report, including all major sections.\"\n",
    ")\n",
    "print(\"\\n\\nStreaming complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 8: Analyzing Specific Pages\n",
    "\n",
    "For our 3-page document, let's analyze the Implementation Guidelines on page 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing page 3 (Implementation Guidelines)...\n",
      "Response:\n",
      "Based on the technical report, here are the implementation guidelines and best practices for deploying language models in production environments:\n",
      "\n",
      "## Model Selection\n",
      "Choose models based on specific use case requirements. Claude 3 excels at complex reasoning, while GPT-3.5 offers optimal speed for simple tasks.\n",
      "\n",
      "## Prompt Engineering\n",
      "Invest in developing clear, structured prompts. Include examples and explicit instructions to improve output quality.\n",
      "\n",
      "## Cost Optimization\n",
      "Implement token counting and caching strategies. Use smaller models for initial filtering before engaging larger models.\n",
      "\n",
      "## Quality Assurance\n",
      "Establish automated testing pipelines with diverse test cases. Monitor model outputs for drift and degradation.\n",
      "\n",
      "## Security Considerations\n",
      "Implement proper input sanitization and output filtering. Never expose raw model outputs without validation.\n",
      "\n",
      "## Technical Infrastructure\n",
      "All models were tested using AWS EC2 p3.2xlarge instances with NVIDIA V100 GPUs, 16GB GPU memory, and optimized inference libraries. API calls were made through AWS Bedrock with standard throttling limits applied.\n",
      "\n",
      "## Key Success Factors\n",
      "With proper implementation strategies, these models can deliver substantial value in automation, analysis, and decision support applications. The key to success lies in understanding model strengths, implementing robust engineering practices, and maintaining continuous evaluation processes.\n",
      "\n",
      "These guidelines emphasize the importance of strategic model selection, robust engineering practices, continuous monitoring, and proper security measures for successful production deployment of language models.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Citations:\n",
      "\n",
      "[1] \"Technical Report TR-2024-001\n",
      "Implementation Guidelines\n",
      "Based on our extensive analysis, we recommend the following best practices for implementing language\n",
      "models in production environments:\n",
      "Model Selection:\n",
      "Choose models based on specific use case requirements. Claude 3 excels at complex reasoning, while\n",
      "GPT-3.5 offers optimal speed for simple tasks.\n",
      "\"\n",
      "    Source: Document (pages 3-3)\n",
      "    Pages: 1-2\n",
      "\n",
      "[2] \"Prompt Engineering:\n",
      "Invest in developing clear, structured prompts. Include examples and explicit instructions to improve output\n",
      "quality.\n",
      "\"\n",
      "    Source: Document (pages 3-3)\n",
      "    Pages: 1-2\n",
      "\n",
      "[3] \"Cost Optimization:\n",
      "Implement token counting and caching strategies. Use smaller models for initial filtering before engaging\n",
      "larger models.\n",
      "\"\n",
      "    Source: Document (pages 3-3)\n",
      "    Pages: 1-2\n",
      "\n",
      "[4] \"Quality Assurance:\n",
      "Establish automated testing pipelines with diverse test cases. Monitor model outputs for drift and\n",
      "degradation.\n",
      "\"\n",
      "    Source: Document (pages 3-3)\n",
      "    Pages: 1-2\n",
      "\n",
      "[5] \"Security Considerations:\n",
      "Implement proper input sanitization and output filtering. Never expose raw model outputs without validation.\n",
      "\"\n",
      "    Source: Document (pages 3-3)\n",
      "    Pages: 1-2\n",
      "\n",
      "[6] \"Technical Specifications\n",
      "All models were tested using the following configuration: AWS EC2 p3.2xlarge instances with NVIDIA V100\n",
      "GPUs, 16GB GPU memory, and optimized inference libraries. API calls were made through AWS Bedrock\n",
      "with standard throttling limits applied.\n",
      "\"\n",
      "    Source: Document (pages 3-3)\n",
      "    Pages: 1-2\n",
      "\n",
      "[7] \"Our analysis demonstrates that with proper implementation strategies, these models can\n",
      "deliver substantial value in automation, analysis, and decision support applications. The key to success lies in\n",
      "understanding model strengths, implementing robust engineering practices, and maintaining continuous\n",
      "evaluation processes.\n",
      "\"\n",
      "    Source: Document (pages 3-3)\n",
      "    Pages: 1-2\n"
     ]
    }
   ],
   "source": [
    "# Analyze only page 3 (Implementation Guidelines)\n",
    "print(\"Analyzing page 3 (Implementation Guidelines)...\")\n",
    "response = analyze_pdf_section(\n",
    "    pdf_path, \n",
    "    2, 3,  # Pages are 0-indexed, so page 3 is index 2\n",
    "    \"Summarize all the implementation guidelines and best practices mentioned on this page.\"\n",
    ")\n",
    "display_response_with_citations(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Techniques <a id=\"advanced\"></a>\n",
    "\n",
    "### Comparing PDF Analysis vs Text Extraction\n",
    "\n",
    "Let's demonstrate why native PDF handling is superior to text extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Asking about visual content with PDF handling:\n",
      "Based on the performance comparison chart on page 2, it shows two key visualizations:\n",
      "\n",
      "## Model Performance Comparison (Bar Chart)\n",
      "This displays performance metrics for four language models:\n",
      "- **GPT-3.5**: ~85% accuracy, ~80 speed score\n",
      "- **Claude 2**: ~88% accuracy, ~85 speed score  \n",
      "- **Claude 3**: ~92% accuracy, ~80 speed score\n",
      "- **GPT-4**: ~91% accuracy, ~75 speed score\n",
      "\n",
      "The chart uses two metrics represented by different colored bars:\n",
      "- **Accuracy %** (shown in blue/teal)\n",
      "- **Speed Score** (shown in purple/magenta)\n",
      "\n",
      "## Token Usage Pattern (Line Chart)\n",
      "This shows token usage over a 30-day period, displaying:\n",
      "- Usage starting around 1,000 tokens (thousands) at the beginning of the month\n",
      "- Peak usage of approximately 1,500 tokens around day 5-10\n",
      "- A general decline through the middle of the month to around 500 tokens\n",
      "- A slight uptick toward the end of the month back to around 750 tokens\n",
      "\n",
      "## Key Insights\n",
      "- **Claude 3** achieved the highest accuracy at 92%\n",
      "- **GPT-3.5** showed the best speed performance\n",
      "- There's generally an inverse relationship between accuracy and speed across models\n",
      "- Token usage shows significant variation over time, which could inform cost planning and resource allocation strategies\n",
      "\n",
      "============================================================\n",
      "\n",
      "2. With text extraction, we lose visual information:\n",
      "Extracted text length: 4059 characters\n",
      "Sample of extracted text:\n",
      "Technical Report TR-2024-001\n",
      "Advanced Language Models:\n",
      "Performance Analysis and\n",
      "Implementation Guidelines\n",
      "AI Research Division\n",
      "Published: June 2025\n",
      "Executive Summary\n",
      "This technical report presents a comprehensive analysis of state-of-the-art language models, focusing on\n",
      "performance metrics, implementation best practices, and practical applications. Our research indicates that\n",
      "modern language models have achieved remarkable capabilities in natural language understanding and\n",
      "generation tasks. Key ...\n",
      "\n",
      "Note: Charts, formatting, and visual elements are lost!\n"
     ]
    }
   ],
   "source": [
    "# Extract text from PDF for comparison\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract plain text from PDF\"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Compare approaches\n",
    "print(\"1. Asking about visual content with PDF handling:\")\n",
    "response = analyze_pdf(pdf_path, \"What does the performance comparison chart show?\")\n",
    "if response:\n",
    "    for block in response.get('content', []):\n",
    "        if block.get('type') == 'text':\n",
    "            print(block.get('text', ''))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"2. With text extraction, we lose visual information:\")\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "print(f\"Extracted text length: {len(extracted_text)} characters\")\n",
    "print(\"Sample of extracted text:\")\n",
    "print(extracted_text[:500] + \"...\")\n",
    "print(\"\\nNote: Charts, formatting, and visual elements are lost!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices Summary\n",
    "\n",
    "### PDF Handling Best Practices\n",
    "\n",
    "1. **File Size Management**\n",
    "   - Large PDFs (>10MB) may need compression\n",
    "   - Extract relevant pages when possible\n",
    "   - Consider splitting very large documents\n",
    "\n",
    "2. **Citation Usage**\n",
    "   - Enable citations for compliance and verification needs\n",
    "   - Use citations when exact references are required\n",
    "   - Citations add minimal overhead to response time\n",
    "\n",
    "3. **Error Handling**\n",
    "   - Always implement retry logic for throttling\n",
    "   - Validate PDF encoding before sending\n",
    "   - Handle malformed PDFs gracefully\n",
    "\n",
    "4. **Performance Optimization**\n",
    "   - Use streaming for better user experience\n",
    "   - Cache responses for frequently accessed documents\n",
    "   - Consider prompt caching for repeated analyses\n",
    "\n",
    "5. **Security Considerations**\n",
    "   - Validate PDF sources before processing\n",
    "   - Be aware of sensitive data in documents\n",
    "   - Implement appropriate access controls\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "AWS Bedrock's native PDF handling with Claude models provides powerful capabilities for document analysis:\n",
    "\n",
    "- **Direct PDF processing** preserves all document information\n",
    "- **Citation extraction** enables precise source attribution\n",
    "- **Visual understanding** allows analysis of charts and diagrams\n",
    "- **Streaming support** provides real-time feedback\n",
    "\n",
    "This notebook has demonstrated how to leverage these features for various use cases, from simple Q&A to comprehensive document analysis.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Try the examples with your own PDFs\n",
    "2. Build specialized analyzers for your document types\n",
    "3. Integrate with other AWS services for complete workflows\n",
    "4. Experiment with different prompt strategies for better results\n",
    "\n",
    "Happy PDF processing! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
